{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "67406890d02d4082b666cc2cddd76be9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f70eb0006c5b4a5f89deba3d32ab1c4b",
              "IPY_MODEL_cf9694dd34a34e55a33e8a801092653d",
              "IPY_MODEL_682de979c46747348a8a6acd07ea5a5d"
            ],
            "layout": "IPY_MODEL_aefb8953a8c4493e926c2c8a803e4c07"
          }
        },
        "f70eb0006c5b4a5f89deba3d32ab1c4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90f444c8d0804d31bb2881314e128f15",
            "placeholder": "​",
            "style": "IPY_MODEL_81f984901e90424697dad755aee4a945",
            "value": "phi3.5-phunction-q5-k-m.gguf: 100%"
          }
        },
        "cf9694dd34a34e55a33e8a801092653d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b426179f0ff4350a532a45ca4ebce70",
            "max": 2715011232,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5c8156ad337545fe9f870d1100c5882a",
            "value": 2715011232
          }
        },
        "682de979c46747348a8a6acd07ea5a5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c636138895d43218dada189f7a28215",
            "placeholder": "​",
            "style": "IPY_MODEL_b1bf0b1e79e942a4b9d5b8055855cdff",
            "value": " 2.72G/2.72G [00:50&lt;00:00, 59.1MB/s]"
          }
        },
        "aefb8953a8c4493e926c2c8a803e4c07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90f444c8d0804d31bb2881314e128f15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81f984901e90424697dad755aee4a945": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b426179f0ff4350a532a45ca4ebce70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c8156ad337545fe9f870d1100c5882a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4c636138895d43218dada189f7a28215": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1bf0b1e79e942a4b9d5b8055855cdff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Project name: Integrating LLaMA Language Model with FLUX Pipeline for Enhanced Image and Text Generation**\n",
        "\n",
        "Contributor: Rajeev singh sisodiya\n",
        "\n",
        "Overview:\n",
        "\n",
        "This project explores the integration of the LLaMA language model with the FLUX pipeline, combining state-of-the-art text and image generation capabilities into a cohesive system. The LLaMA model, known for its powerful natural language processing and text generation, is used in conjunction with the FLUX pipeline, a diffusion-based image synthesis tool. The goal of this integration is to enable seamless interaction between text and visual content, allowing for the generation of contextually relevant images based on textual descriptions."
      ],
      "metadata": {
        "id": "mlP6Bcn71O3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.FLUX Pipeline for High-Quality Image Generation with Advanced Latent Variable Manipulation:\n",
        "\n",
        "The below provided code defines a process for generating images using a deep learning pipeline with PyTorch and the Diffusers library. The key components of the code include helper functions, input validation, encoding prompts, and preparing latent variables, which are integral to the image generation process."
      ],
      "metadata": {
        "id": "xM6jaK16faKG"
      }
    },
    {
      "source": [
        "!pip install diffusers"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_qCrtRUBDPS4",
        "outputId": "c8c66d22-92c8-49f9-f243-a513a618bf3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (0.30.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (8.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.23.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.4.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (9.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (4.12.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from diffusers import FluxPipeline, AutoencoderTiny, FlowMatchEulerDiscreteScheduler\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "\n",
        "# Helper functions\n",
        "def calculate_shift(\n",
        "    image_seq_len,\n",
        "    base_seq_len: int = 256,\n",
        "    max_seq_len: int = 4096,\n",
        "    base_shift: float = 0.5,\n",
        "    max_shift: float = 1.16,\n",
        "):\n",
        "    m = (max_shift - base_shift) / (max_seq_len - base_seq_len)\n",
        "    b = base_shift - m * base_seq_len\n",
        "    mu = image_seq_len * m + b\n",
        "    return mu\n",
        "\n",
        "def retrieve_timesteps(\n",
        "    scheduler,\n",
        "    num_inference_steps: Optional[int] = None,\n",
        "    device: Optional[Union[str, torch.device]] = None,\n",
        "    timesteps: Optional[List[int]] = None,\n",
        "    sigmas: Optional[List[float]] = None,\n",
        "    **kwargs,\n",
        "):\n",
        "    if timesteps is not None and sigmas is not None:\n",
        "        raise ValueError(\"Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values\")\n",
        "    if timesteps is not None:\n",
        "        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n",
        "        timesteps = scheduler.timesteps\n",
        "        num_inference_steps = len(timesteps)\n",
        "    elif sigmas is not None:\n",
        "        scheduler.set_timesteps(sigmas=sigmas, device=device, **kwargs)\n",
        "        timesteps = scheduler.timesteps\n",
        "        num_inference_steps = len(timesteps)\n",
        "    else:\n",
        "        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)\n",
        "        timesteps = scheduler.timesteps\n",
        "    return timesteps, num_inference_steps\n",
        "\n",
        "\n",
        "# FLUX pipeline function\n",
        "@torch.inference_mode()\n",
        "def flux_pipe_call_that_returns_an_iterable_of_images(\n",
        "    self,\n",
        "    prompt: Union[str, List[str]] = None,\n",
        "    prompt_2: Optional[Union[str, List[str]]] = None,\n",
        "    height: Optional[int] = None,\n",
        "    width: Optional[int] = None,\n",
        "    num_inference_steps: int = 28,\n",
        "    timesteps: List[int] = None,\n",
        "    guidance_scale: float = 3.5,\n",
        "    num_images_per_prompt: Optional[int] = 1,\n",
        "    generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
        "    latents: Optional[torch.FloatTensor] = None,\n",
        "    prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "    pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "    output_type: Optional[str] = \"pil\",\n",
        "    return_dict: bool = True,\n",
        "    joint_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
        "    max_sequence_length: int = 512,\n",
        "    good_vae: Optional[Any] = None,\n",
        "):\n",
        "    height = height or self.default_sample_size * self.vae_scale_factor\n",
        "    width = width or self.default_sample_size * self.vae_scale_factor\n",
        "\n",
        "    # 1. Check inputs\n",
        "    self.check_inputs(\n",
        "        prompt,\n",
        "        prompt_2,\n",
        "        height,\n",
        "        width,\n",
        "        prompt_embeds=prompt_embeds,\n",
        "        pooled_prompt_embeds=pooled_prompt_embeds,\n",
        "        max_sequence_length=max_sequence_length,\n",
        "    )\n",
        "\n",
        "    self._guidance_scale = guidance_scale\n",
        "    self._joint_attention_kwargs = joint_attention_kwargs\n",
        "    self._interrupt = False\n",
        "\n",
        "# 2. Define call parameters\n",
        "    batch_size = 1 if isinstance(prompt, str) else len(prompt)\n",
        "    device = self._execution_device\n",
        "\n",
        "    # 3. Encode prompt\n",
        "    lora_scale = joint_attention_kwargs.get(\"scale\", None) if joint_attention_kwargs is not None else None\n",
        "    prompt_embeds, pooled_prompt_embeds, text_ids = self.encode_prompt(\n",
        "        prompt=prompt,\n",
        "        prompt_2=prompt_2,\n",
        "        prompt_embeds=prompt_embeds,\n",
        "        pooled_prompt_embeds=pooled_prompt_embeds,\n",
        "        device=device,\n",
        "        num_images_per_prompt=num_images_per_prompt,\n",
        "        max_sequence_length=max_sequence_length,\n",
        "        lora_scale=lora_scale,\n",
        "    )\n",
        "    # 4. Prepare latent variables\n",
        "    num_channels_latents = self.transformer.config.in_channels // 4\n",
        "    latents, latent_image_ids = self.prepare_latents(\n",
        "        batch_size * num_images_per_prompt,\n",
        "        num_channels_latents,\n",
        "        height,\n",
        "        width,\n",
        "        prompt_embeds.dtype,\n",
        "        device,\n",
        "        generator,\n",
        "        latents,\n",
        "    )\n",
        "    # 5. Prepare timesteps\n",
        "    sigmas = np.linspace(1.0, 1 / num_inference_steps, num_inference_steps)\n",
        "    image_seq_len = latents.shape[1]\n",
        "    mu = calculate_shift(\n",
        "        image_seq_len,\n",
        "        self.scheduler.config.base_image_seq_len,\n",
        "        self.scheduler.config.max_image_seq_len,\n",
        "        self.scheduler.config.base_shift,\n",
        "        self.scheduler.config.max_shift,\n",
        "    )\n",
        "    timesteps, num_inference_steps = retrieve_timesteps(\n",
        "        self.scheduler,\n",
        "        num_inference_steps,\n",
        "        device,\n",
        "        timesteps,\n",
        "        sigmas,\n",
        "        mu=mu,\n",
        "    )\n",
        "    self._num_timesteps = len(timesteps)\n",
        "\n",
        "    # Handle guidance\n",
        "    guidance = torch.full([1], guidance_scale, device=device, dtype=torch.float32).expand(latents.shape[0]) if self.transformer.config.guidance_embeds else None\n",
        "\n",
        "    # 6. Denoising loop\n",
        "    for i, t in enumerate(timesteps):\n",
        "        if self.interrupt:\n",
        "            continue\n",
        "\n",
        "        timestep = t.expand(latents.shape[0]).to(latents.dtype)\n",
        "\n",
        "        noise_pred = self.transformer(\n",
        "            hidden_states=latents,\n",
        "            timestep=timestep / 1000,\n",
        "            guidance=guidance,\n",
        "            pooled_projections=pooled_prompt_embeds,\n",
        "            encoder_hidden_states=prompt_embeds,\n",
        "            txt_ids=text_ids,\n",
        "            img_ids=latent_image_ids,\n",
        "            joint_attention_kwargs=self.joint_attention_kwargs,\n",
        "            return_dict=False,\n",
        "        )[0]\n",
        "        latents = self.scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n",
        "\n",
        "# Yield intermediate result\n",
        "        latents_for_image = self._unpack_latents(latents, height, width, self.vae_scale_factor)\n",
        "        latents_for_image = (latents_for_image / self.vae.config.scaling_factor) + self.vae.config.shift_factor\n",
        "        image = self.vae.decode(latents_for_image, return_dict=False)[0]\n",
        "        yield self.image_processor.postprocess(image, output_type=output_type)[0]\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Final image using good_vae\n",
        "    latents = self._unpack_latents(latents, height, width, self.vae_scale_factor)\n",
        "    latents = (latents / good_vae.config.scaling_factor) + good_vae.config.shift_factor\n",
        "    image = good_vae.decode(latents, return_dict=False)[0]\n",
        "    self.maybe_free_model_hooks()\n",
        "    torch.cuda.empty_cache()\n",
        "    yield self.image_processor.postprocess(image, output_type=output_type)[0]\n",
        "\n"
      ],
      "metadata": {
        "id": "CUm14FYgDuHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion:\n",
        "Above code outlines a robust and modular approach to image generation using a diffusion-based pipeline. By leveraging PyTorch and the Diffusers library, it enables detailed control over the generation process, including guidance, timesteps, and latent variable manipulation. The modular design, including helper functions and a clearly defined pipeline function, allows for flexibility and potential customization for various image generation tasks. The final output is an iterable of images, generated step by step, ensuring high-quality results through iterative refinement."
      ],
      "metadata": {
        "id": "RuwU_f_JftOj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0FzLM58qHTIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Integrating LLaMA Language Model with FLUX Pipeline for Enhanced Image and Text Generation\n",
        "\n"
      ],
      "metadata": {
        "id": "8koF32_Jjdrq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This belo code demonstrates a powerful combination of natural language processing and image generation using state-of-the-art models. The integration features the LLaMA language model, known for its robust text generation capabilities, alongside the FLUX image generation pipeline, which leverages diffusion-based models to produce high-quality images.\n",
        "\n",
        "The workflow is designed to perform both text and image generation in a unified pipeline. The LLaMA model is first initialized to handle natural language tasks, while the FLUX pipeline manages the diffusion process to create images based on prompts. This integration allows for sophisticated interactions between text and visual content, offering a seamless approach to generating rich multimedia outputs. The code also includes helper functions for computing shifts and managing timesteps, ensuring the smooth operation of the overall process.\n",
        "\n"
      ],
      "metadata": {
        "id": "LFBTPCYpj5KC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Gl6PydNPXT8n",
        "outputId": "a2290ea5-67e6-4732-bee9-10ae7a04e0cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.90.tar.gz (63.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.26.4)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m167.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.90-cp310-cp310-linux_x86_64.whl size=3398639 sha256=31e2dab792b79c00d1aa7a7aeff0264aa8334159f658f3dca4e0dad298619482\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/67/02/f950031435db4a5a02e6269f6adb6703bf1631c3616380f3c6\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from diffusers import FluxPipeline, AutoencoderTiny, FlowMatchEulerDiscreteScheduler\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "from llama_cpp import Llama  # Added import for Llama model\n",
        "\n",
        "# Initialize Llama model with the correct filename\n",
        "llm = Llama.from_pretrained(\n",
        "    repo_id=\"ayan-sh003/phi3.5-phunction-calling-GGUF\",\n",
        "    filename=\"phi3.5-phunction-q5-k-m.gguf\", # Use the correct filename from the available files\n",
        ")\n",
        "\n",
        "# Helper functions\n",
        "def calculate_shift(\n",
        "    image_seq_len: int,\n",
        "    base_seq_len: int = 256,\n",
        "    max_seq_len: int = 4096,\n",
        "    base_shift: float = 0.5,\n",
        "    max_shift: float = 1.16,\n",
        ") -> float:\n",
        "    m = (max_shift - base_shift) / (max_seq_len - base_seq_len)\n",
        "    b = base_shift - m * base_seq_len\n",
        "    mu = image_seq_len * m + b\n",
        "    return mu\n",
        "\n",
        "def retrieve_timesteps(\n",
        "    scheduler: Any,\n",
        "    num_inference_steps: Optional[int] = None,\n",
        "    device: Optional[Union[str, torch.device]] = None,\n",
        "    timesteps: Optional[List[int]] = None,\n",
        "    sigmas: Optional[List[float]] = None,\n",
        "    **kwargs,\n",
        ") -> (torch.Tensor, int):\n",
        "    if timesteps is not None and sigmas is not None:\n",
        "        raise ValueError(\"Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values\")\n",
        "    if timesteps is not None:\n",
        "        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n",
        "        timesteps = scheduler.timesteps\n",
        "        num_inference_steps = len(timesteps)\n",
        "    elif sigmas is not None:\n",
        "        scheduler.set_timesteps(sigmas=sigmas, device=device, **kwargs)\n",
        "        timesteps = scheduler.timesteps\n",
        "        num_inference_steps = len(timesteps)\n",
        "    else:\n",
        "        scheduler.set_timesteps(num_inference_steps=num_inference_steps, device=device, **kwargs)\n",
        "        timesteps = scheduler.timesteps\n",
        "    return timesteps, num_inference_steps\n",
        "\n",
        "# FLUX pipeline function\n",
        "@torch.inference_mode()\n",
        "def flux_pipe_call_that_returns_an_iterable_of_images(\n",
        "    self,\n",
        "    prompt: Union[str, List[str]] = None,\n",
        "    prompt_2: Optional[Union[str, List[str]]] = None,\n",
        "    height: Optional[int] = None,\n",
        "    width: Optional[int] = None,\n",
        "    num_inference_steps: int = 28,\n",
        "    timesteps: List[int] = None,\n",
        "    guidance_scale: float = 3.5,\n",
        "    num_images_per_prompt: Optional[int] = 1,\n",
        "    generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
        "    latents: Optional[torch.FloatTensor] = None,\n",
        "    prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "    pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "    output_type: Optional[str] = \"pil\",\n",
        "    return_dict: bool = True,\n",
        "    joint_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
        "    max_sequence_length: int = 512,\n",
        "    good_vae: Optional[Any] = None,\n",
        "):\n",
        "    height = height or self.default_sample_size * self.vae_scale_factor\n",
        "    width = width or self.default_sample_size * self.vae_scale_factor\n",
        "\n",
        "    # 1. Check inputs\n",
        "    self.check_inputs(\n",
        "        prompt,\n",
        "        prompt_2,\n",
        "        height,\n",
        "        width,\n",
        "        prompt_embeds=prompt_embeds,\n",
        "        pooled_prompt_embeds=pooled_prompt_embeds,\n",
        "        max_sequence_length=max_sequence_length,\n",
        "    )\n",
        "\n",
        "    self._guidance_scale = guidance_scale\n",
        "    self._joint_attention_kwargs = joint_attention_kwargs\n",
        "    self._interrupt = False\n",
        "\n",
        "    # 2. Define call parameters\n",
        "    batch_size = 1 if isinstance(prompt, str) else len(prompt)\n",
        "    device = self._execution_device\n",
        "\n",
        "    # 3. Encode prompt\n",
        "    lora_scale = joint_attention_kwargs.get(\"scale\", None) if joint_attention_kwargs is not None else None\n",
        "    prompt_embeds, pooled_prompt_embeds, text_ids = self.encode_prompt(\n",
        "        prompt=prompt,\n",
        "        prompt_2=prompt_2,\n",
        "        prompt_embeds=prompt_embeds,\n",
        "        pooled_prompt_embeds=pooled_prompt_embeds,\n",
        "        device=device,\n",
        "        num_images_per_prompt=num_images_per_prompt,\n",
        "        max_sequence_length=max_sequence_length,\n",
        "        lora_scale=lora_scale,\n",
        "    )\n",
        "\n",
        "    # 4. Prepare latent variables\n",
        "    num_channels_latents = self.transformer.config.in_channels // 4\n",
        "    latents, latent_image_ids = self.prepare_latents(\n",
        "        batch_size * num_images_per_prompt,\n",
        "        num_channels_latents,\n",
        "        height,\n",
        "        width,\n",
        "        prompt_embeds.dtype,\n",
        "        device,\n",
        "        generator,\n",
        "        latents,\n",
        "    )\n",
        "\n",
        "    # 5. Prepare timesteps\n",
        "    sigmas = np.linspace(1.0, 1 / num_inference_steps, num_inference_steps)\n",
        "    image_seq_len = latents.shape[1]\n",
        "    mu = calculate_shift(\n",
        "        image_seq_len,\n",
        "        self.scheduler.config.base_image_seq_len,\n",
        "        self.scheduler.config.max_image_seq_len,\n",
        "        self.scheduler.config.base_shift,\n",
        "        self.scheduler.config.max_shift,\n",
        "    )\n",
        "    timesteps, num_inference_steps = retrieve_timesteps(\n",
        "        self.scheduler,\n",
        "        num_inference_steps,\n",
        "        device,\n",
        "        timesteps,\n",
        "        sigmas,\n",
        "        mu=mu,\n",
        "    )\n",
        "    self._num_timesteps = len(timesteps)\n",
        "\n",
        "    # 6. Llama model inference\n",
        "    llama_response = llm.create_chat_completion(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"What is the capital of France?\"\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "    print(\"Llama Response:\", llama_response['choices'][0]['message']['content'])\n",
        "\n",
        "    # 7. Handle guidance\n",
        "    guidance = (\n",
        "        torch.full([1], guidance_scale, device=device, dtype=torch.float32).expand(latents.shape[0])\n",
        "        if self.transformer.config.guidance_embeds\n",
        "        else None\n",
        "    )\n",
        "\n",
        "    # 8. Denoising loop\n",
        "    for i, t in enumerate(timesteps):\n",
        "        if self._interrupt:\n",
        "            break\n",
        "\n",
        "        timestep = t.expand(latents.shape[0]).to(latents.dtype)\n",
        "\n",
        "        noise_pred = self.transformer(\n",
        "            hidden_states=latents,\n",
        "            timestep=timestep / 1000,\n",
        "            guidance=guidance,\n",
        "            pooled_projections=pooled_prompt_embeds,\n",
        "            encoder_hidden_states=prompt_embeds,\n",
        "            txt_ids=text_ids,\n",
        "            img_ids=latent_image_ids,\n",
        "            joint_attention_kwargs=self.joint_attention_kwargs,\n",
        "            return_dict=False,\n",
        "        )[0]\n",
        "        latents = self.scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n",
        "\n",
        "        # Yield intermediate result\n",
        "        latents_for_image = self._unpack_latents(latents, height, width, self.vae_scale_factor)\n",
        "        latents_for_image = (latents_for_image / self.vae.config.scaling_factor) + self.vae.config.shift_factor\n",
        "        image = self.vae.decode(latents_for_image, return_dict=False)[0]\n",
        "        yield self.image_processor.postprocess(image, output_type=output_type)[0]\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # 9. Final image using good_vae\n",
        "    latents = self._unpack_latents(latents, height, width, self.vae_scale_factor)\n",
        "    latents = (latents / good_vae.config.scaling_factor) + good_vae.config.shift_factor\n",
        "    image = good_vae.decode(latents, return_dict=False)[0]\n",
        "    self.maybe_free_model_hooks()\n",
        "    torch.cuda.empty_cache()\n",
        "    yield self.image_processor.postprocess(image, output_type=output_type)[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "67406890d02d4082b666cc2cddd76be9",
            "f70eb0006c5b4a5f89deba3d32ab1c4b",
            "cf9694dd34a34e55a33e8a801092653d",
            "682de979c46747348a8a6acd07ea5a5d",
            "aefb8953a8c4493e926c2c8a803e4c07",
            "90f444c8d0804d31bb2881314e128f15",
            "81f984901e90424697dad755aee4a945",
            "5b426179f0ff4350a532a45ca4ebce70",
            "5c8156ad337545fe9f870d1100c5882a",
            "4c636138895d43218dada189f7a28215",
            "b1bf0b1e79e942a4b9d5b8055855cdff"
          ]
        },
        "id": "7a5XNU2aYMlp",
        "outputId": "fc4d1424-3f68-4c6e-9c52-533290701216"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "phi3.5-phunction-q5-k-m.gguf:   0%|          | 0.00/2.72G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67406890d02d4082b666cc2cddd76be9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /root/.cache/huggingface/hub/models--ayan-sh003--phi3.5-phunction-calling-GGUF/snapshots/d2748d5f380f85ced5fd21033cf4490490ad7da8/./phi3.5-phunction-q5-k-m.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Phi 3.5 Mini Instruct\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = instruct\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Phi-3.5\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = mini\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                          general.file_type u32              = 17\n",
            "llama_model_loader: - kv  16:                           llama.vocab_size u32              = 32064\n",
            "llama_model_loader: - kv  17:                 llama.rope.dimension_count u32              = 96\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 32009\n",
            "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if 'role' in messages[0] %}{% for ...\n",
            "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q5_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens cache size = 14\n",
            "llm_load_vocab: token to piece cache size = 0.1685 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32064\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 3072\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_rot            = 96\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 96\n",
            "llm_load_print_meta: n_embd_head_v    = 96\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
            "llm_load_print_meta: n_embd_v_gqa     = 3072\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 8192\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
            "llm_load_print_meta: model params     = 3.82 B\n",
            "llm_load_print_meta: model size       = 2.53 GiB (5.68 BPW) \n",
            "llm_load_print_meta: general.name     = Phi 3.5 Mini Instruct\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 32009 '<|placeholder6|>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: EOT token        = 32007 '<|end|>'\n",
            "llm_load_print_meta: max token length = 48\n",
            "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
            "llm_load_tensors:        CPU buffer size =  2588.53 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   192.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  192.00 MiB, K (f16):   96.00 MiB, V (f16):   96.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    68.63 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{% if 'role' in messages[0] %}{% for message in messages %}{% if message['role'] == 'user' %}{{'<|user|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\\n' + message['content'] + '<|end|>\\n'}}{% else %}{{'<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\\n' }}{% endif %}{% else %}{% for message in messages %}{% if message['from'] == 'human' %}{{'<|user|>\\n' + message['value'] + '<|end|>\\n'}}{% elif message['from'] == 'gpt' %}{{'<|assistant|>\\n' + message['value'] + '<|end|>\\n'}}{% else %}{{'<|' + message['from'] + '|>\\n' + message['value'] + '<|end|>\\n'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\\n' }}{% endif %}{% endif %}\", 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_space_prefix': 'false', 'llama.rope.dimension_count': '96', 'llama.vocab_size': '32064', 'general.file_type': '17', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'general.architecture': 'llama', 'tokenizer.ggml.padding_token_id': '32009', 'general.basename': 'Phi-3.5', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'default', 'llama.context_length': '131072', 'general.name': 'Phi 3.5 Mini Instruct', 'general.organization': 'Unsloth', 'general.finetune': 'instruct', 'general.type': 'model', 'general.size_label': 'mini', 'tokenizer.ggml.add_bos_token': 'false', 'llama.embedding_length': '3072', 'llama.feed_forward_length': '8192', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {% if 'role' in messages[0] %}{% for message in messages %}{% if message['role'] == 'user' %}{{'<|user|>\n",
            "' + message['content'] + '<|end|>\n",
            "'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n",
            "' + message['content'] + '<|end|>\n",
            "'}}{% else %}{{'<|' + message['role'] + '|>\n",
            "' + message['content'] + '<|end|>\n",
            "'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
            "' }}{% endif %}{% else %}{% for message in messages %}{% if message['from'] == 'human' %}{{'<|user|>\n",
            "' + message['value'] + '<|end|>\n",
            "'}}{% elif message['from'] == 'gpt' %}{{'<|assistant|>\n",
            "' + message['value'] + '<|end|>\n",
            "'}}{% else %}{{'<|' + message['from'] + '|>\n",
            "' + message['value'] + '<|end|>\n",
            "'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
            "' }}{% endif %}{% endif %}\n",
            "Using chat eos_token: <|endoftext|>\n",
            "Using chat bos_token: <s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Summary:\n",
        "The process begins with the successful download and loading of the Phi 3.5 Mini Instruct model, which is a LLaMA-based model stored in the GGUF V3 format. The model metadata consists of 32 key-value pairs and 291 tensors, indicating various parameters and settings that define the model’s architecture, attention mechanisms, tokenizer settings, and more. The download was completed at a speed of 59.1MB/s, indicating the model size of 2.72 GB.\n",
        "\n",
        "Key metadata highlights include:\n",
        "\n",
        "Model Architecture: LLaMA with 32 layers, 32 attention heads, and a feed-forward length of 8192.\n",
        "\n",
        "Context Length: 131072 tokens, allowing for long input sequences.\n",
        "\n",
        "Embedding and Attention Parameters: Embedding length of 3072, with 96 dimensions for rotary positional embeddings (ROPE).\n",
        "\n",
        "Tokenization Details: The vocabulary size is 32064, with specific tokens for beginning of sequence (BOS), end of sequence (EOS), unknown tokens (UNK), and padding.\n",
        "\n",
        "Quantization: The model uses Q5_K quantization, with a total size of 2.53 GiB.\n",
        "\n",
        "After the model is loaded, the initialization of the context for inference is shown, including the allocation of memory buffers and the configuration of compute resources.\n",
        "\n",
        "# Conclusion:\n",
        "The Phi 3.5 Mini Instruct model, based on the LLaMA architecture, has been successfully loaded and initialized with detailed configuration parameters. The metadata provides insight into the model's structure, tokenization scheme, and memory usage, emphasizing its capability to handle complex tasks with a significant context length. The process demonstrated the efficiency of the model's setup, with quantization enabling a compact model size while retaining high performance. This positions the model as a powerful tool for natural language processing tasks that require detailed instruction-following capabilities."
      ],
      "metadata": {
        "id": "5VeCvNAId8Kl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Advanced Image Generation Using the FLUX Pipeline with Custom Timesteps and Latent Variable Handling\n",
        "\n",
        "Below code provides a comprehensive solution for generating high-quality images through a diffusion-based approach using the FLUX pipeline. By leveraging PyTorch and the Diffusers library, the pipeline facilitates intricate control over the image generation process, including the management of timesteps, latent variables, and model guidance. The code includes helper functions for calculating shifts and retrieving timesteps, ensuring that the image synthesis process is both flexible and precise. Designed for scalability, the pipeline can handle complex input prompts and generate multiple images per prompt with customizable parameters, making it suitable for a wide range of generative tasks."
      ],
      "metadata": {
        "id": "6HjYIgruyBqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from diffusers import FluxPipeline, AutoencoderTiny, FlowMatchEulerDiscreteScheduler\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "\n",
        "# Helper functions\n",
        "def calculate_shift(\n",
        "    image_seq_len: int,\n",
        "    base_seq_len: int = 256,\n",
        "    max_seq_len: int = 4096,\n",
        "    base_shift: float = 0.5,\n",
        "    max_shift: float = 1.16,\n",
        ") -> float:\n",
        "    m = (max_shift - base_shift) / (max_seq_len - base_seq_len)\n",
        "    b = base_shift - m * base_seq_len\n",
        "    mu = image_seq_len * m + b\n",
        "    return mu\n",
        "\n",
        "def retrieve_timesteps(\n",
        "    scheduler: Any,\n",
        "    num_inference_steps: Optional[int] = None,\n",
        "    device: Optional[Union[str, torch.device]] = None,\n",
        "    timesteps: Optional[List[int]] = None,\n",
        "    sigmas: Optional[List[float]] = None,\n",
        "    **kwargs,\n",
        ") -> (torch.Tensor, int):\n",
        "    if timesteps is not None and sigmas is not None:\n",
        "        raise ValueError(\"Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values.\")\n",
        "    if timesteps is not None:\n",
        "        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n",
        "        timesteps = scheduler.timesteps\n",
        "        num_inference_steps = len(timesteps)\n",
        "    elif sigmas is not None:\n",
        "        scheduler.set_timesteps(sigmas=sigmas, device=device, **kwargs)\n",
        "        timesteps = scheduler.timesteps\n",
        "        num_inference_steps = len(timesteps)\n",
        "    else:\n",
        "        scheduler.set_timesteps(num_inference_steps=num_inference_steps, device=device, **kwargs)\n",
        "        timesteps = scheduler.timesteps\n",
        "    return timesteps, num_inference_steps\n",
        "\n",
        "# FLUX pipeline function\n",
        "@torch.inference_mode()\n",
        "def flux_pipe_call_that_returns_an_iterable_of_images(\n",
        "    self,\n",
        "    prompt: Union[str, List[str]] = None,\n",
        "    prompt_2: Optional[Union[str, List[str]]] = None,\n",
        "    height: Optional[int] = None,\n",
        "    width: Optional[int] = None,\n",
        "    num_inference_steps: int = 28,\n",
        "    timesteps: List[int] = None,\n",
        "    guidance_scale: float = 3.5,\n",
        "    num_images_per_prompt: Optional[int] = 1,\n",
        "    generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
        "    latents: Optional[torch.FloatTensor] = None,\n",
        "    prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "    pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "    output_type: Optional[str] = \"pil\",\n",
        "    return_dict: bool = True,\n",
        "    joint_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
        "    max_sequence_length: int = 512,\n",
        "    good_vae: Optional[Any] = None,\n",
        "):\n",
        "    height = height or self.default_sample_size * self.vae_scale_factor\n",
        "    width = width or self.default_sample_size * self.vae_scale_factor\n",
        "\n",
        "    # 1. Check inputs\n",
        "    self.check_inputs(\n",
        "        prompt,\n",
        "        prompt_2,\n",
        "        height,\n",
        "        width,\n",
        "        prompt_embeds=prompt_embeds,\n",
        "        pooled_prompt_embeds=pooled_prompt_embeds,\n",
        "        max_sequence_length=max_sequence_length,\n",
        "    )\n",
        "\n",
        "    self._guidance_scale = guidance_scale\n",
        "    self._joint_attention_kwargs = joint_attention_kwargs\n",
        "    self._interrupt = False\n",
        "\n",
        "    # 2. Define call parameters\n",
        "    batch_size = 1 if isinstance(prompt, str) else len(prompt)\n",
        "    device = self._execution_device\n",
        "\n",
        "    # 3. Encode prompt\n",
        "    lora_scale = joint_attention_kwargs.get(\"scale\", None) if joint_attention_kwargs is not None else None\n",
        "    prompt_embeds, pooled_prompt_embeds, text_ids = self.encode_prompt(\n",
        "        prompt=prompt,\n",
        "        prompt_2=prompt_2,\n",
        "        prompt_embeds=prompt_embeds,\n",
        "        pooled_prompt_embeds=pooled_prompt_embeds,\n",
        "        device=device,\n",
        "        num_images_per_prompt=num_images_per_prompt,\n",
        "        max_sequence_length=max_sequence_length,\n",
        "        lora_scale=lora_scale,\n",
        "    )\n",
        "\n",
        "    # 4. Prepare latent variables\n",
        "    num_channels_latents = self.transformer.config.in_channels // 4\n",
        "    latents, latent_image_ids = self.prepare_latents(\n",
        "        batch_size * num_images_per_prompt,\n",
        "        num_channels_latents,\n",
        "        height,\n",
        "        width,\n",
        "        prompt_embeds.dtype,\n",
        "        device,\n",
        "        generator,\n",
        "        latents,\n",
        "    )\n",
        "\n",
        "    # 5. Prepare timesteps\n",
        "    sigmas = np.linspace(1.0, 1 / num_inference_steps, num_inference_steps)\n",
        "    image_seq_len = latents.shape[1]\n",
        "    mu = calculate_shift(\n",
        "        image_seq_len,\n",
        "        self.scheduler.config.base_image_seq_len,\n",
        "        self.scheduler.config.max_image_seq_len,\n",
        "        self.scheduler.config.base_shift,\n",
        "        self.scheduler.config.max_shift,\n",
        "    )\n",
        "    timesteps, num_inference_steps = retrieve_timesteps(\n",
        "        self.scheduler,\n",
        "        num_inference_steps,\n",
        "        device,\n",
        "        timesteps,\n",
        "        sigmas,\n",
        "        mu=mu,\n",
        "    )\n",
        "    self._num_timesteps = len(timesteps)\n",
        "\n",
        "    # Handle guidance\n",
        "    guidance = (\n",
        "        torch.full([1], guidance_scale, device=device, dtype=torch.float32)\n",
        "        .expand(latents.shape[0])\n",
        "        if self.transformer.config.guidance_embeds\n",
        "        else None\n",
        "    )\n",
        "\n",
        "    # 6. Denoising loop\n",
        "    for i, t in enumerate(timesteps):\n",
        "        if self._interrupt:\n",
        "            break\n",
        "\n",
        "        timestep = t.expand(latents.shape[0]).to(latents.dtype)\n",
        "\n",
        "        noise_pred = self.transformer(\n",
        "            hidden_states=latents,\n",
        "            timestep=timestep / 1000,\n",
        "            guidance=guidance,\n",
        "            pooled_projections=pooled_prompt_embeds,\n",
        "            encoder_hidden_states=prompt_embeds,\n",
        "            txt_ids=text_ids,\n",
        "            img_ids=latent_image_ids,\n",
        "            joint_attention_kwargs=self.joint_attention_kwargs,\n",
        "            return_dict=False,\n",
        "        )[0]\n",
        "\n",
        "        latents = self.scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n",
        "\n",
        "        # Yield intermediate result\n",
        "        latents_for_image = self._unpack_latents(latents, height, width, self.vae_scale_factor)\n",
        "        latents_for_image = (latents_for_image / self.vae.config.scaling_factor) + self.vae.config.shift_factor\n",
        "        image = self.vae.decode(latents_for_image, return_dict=False)[0]\n",
        "        yield self.image_processor.postprocess(image, output_type=output_type)[0]\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Final image using good_vae\n",
        "    latents = self._unpack_latents(latents, height, width, self.vae_scale_factor)\n",
        "    latents = (latents / good_vae.config.scaling_factor) + good_vae.config.shift_factor\n",
        "    image = good_vae.decode(latents, return_dict=False)[0]\n",
        "    self.maybe_free_model_hooks()\n",
        "    torch.cuda.empty_cache()\n",
        "    yield self.image_processor.postprocess(image, output_type=output_type)[0]\n"
      ],
      "metadata": {
        "id": "24rDVTzIh2Hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Combining LLaMA Language Model with FLUX Pipeline for Integrated Text and Image Generation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CwMuNuLVkiFD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below provided code details the loading and initialization of the Phi 3.5 Mini Instruct model using the LLaMA architecture from a GGUF (GGML Universal File) format. The model metadata includes various configurations and settings that describe the model’s architecture, tokenizer settings, attention mechanisms, and file format details.\n",
        "\n",
        "Key points include:\n",
        "\n",
        "Model Metadata: The metadata consists of 32 key-value pairs and 291 tensors, detailing aspects like model type (llama), context length (131072), embedding length (3072), and attention head count (32).\n",
        "\n",
        "Model Loading: The model is successfully loaded, with specific mentions of parameters like block count, vocabulary size (32064), and quantization version (2).\n",
        "\n",
        "Chat Template: The chat formatting uses a template to structure the input and output during inference, indicating how user and assistant roles are handled.\n",
        "\n",
        "Performance Metrics: The output includes timings for different phases like model loading (5241.34 ms), sampling (0.06 ms per token), and evaluation (3436.32 ms for 7 runs). The total time for processing 20 tokens is approximately 8685.58 ms.\n",
        "\n",
        "Inference Result: The model correctly responds to the prompt \"What is the capital of France?\" with \"The capital of France is Paris.\"\n"
      ],
      "metadata": {
        "id": "MB5YsWC6iSHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from diffusers import FluxPipeline, AutoencoderTiny, FlowMatchEulerDiscreteScheduler\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "from llama_cpp import Llama\n",
        "\n",
        "# Helper functions\n",
        "def calculate_shift(\n",
        "    image_seq_len,\n",
        "    base_seq_len: int = 256,\n",
        "    max_seq_len: int = 4096,\n",
        "    base_shift: float = 0.5,\n",
        "    max_shift: float = 1.16,\n",
        "):\n",
        "    m = (max_shift - base_shift) / (max_seq_len - base_seq_len)\n",
        "    b = base_shift - m * base_seq_len\n",
        "    mu = image_seq_len * m + b\n",
        "    return mu\n",
        "\n",
        "def retrieve_timesteps(\n",
        "    scheduler,\n",
        "    num_inference_steps: Optional[int] = None,\n",
        "    device: Optional[Union[str, torch.device]] = None,\n",
        "    timesteps: Optional[List[int]] = None,\n",
        "    sigmas: Optional[List[float]] = None,\n",
        "    **kwargs,\n",
        "):\n",
        "    if timesteps is not None and sigmas is not None:\n",
        "        raise ValueError(\"Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values\")\n",
        "    if timesteps is not None:\n",
        "        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n",
        "        timesteps = scheduler.timesteps\n",
        "        num_inference_steps = len(timesteps)\n",
        "    elif sigmas is not None:\n",
        "        scheduler.set_timesteps(sigmas=sigmas, device=device, **kwargs)\n",
        "        timesteps = scheduler.timesteps\n",
        "        num_inference_steps = len(timesteps)\n",
        "    else:\n",
        "        scheduler.set_timesteps(num_inference_steps=num_inference_steps, device=device, **kwargs)\n",
        "        timesteps = scheduler.timesteps\n",
        "    return timesteps, num_inference_steps\n",
        "\n",
        "# Initialize the Llama model\n",
        "# Replace 'GGUF_FILE' with the actual filename from the available files\n",
        "llm = Llama.from_pretrained(\n",
        "    repo_id=\"ayan-sh003/phi3.5-phunction-calling-GGUF\",\n",
        "    filename=\"phi3.5-phunction-q5-k-m.gguf\",  # Use the correct filename\n",
        ")\n",
        "\n",
        "# FLUX pipeline function\n",
        "@torch.inference_mode()\n",
        "def flux_pipe_call_that_returns_an_iterable_of_images(\n",
        "    self,\n",
        "    prompt: Union[str, List[str]] = None,\n",
        "    prompt_2: Optional[Union[str, List[str]]] = None,\n",
        "    height: Optional[int] = None,\n",
        "    width: Optional[int] = None,\n",
        "    num_inference_steps: int = 28,\n",
        "    timesteps: List[int] = None,\n",
        "    guidance_scale: float = 3.5,\n",
        "    num_images_per_prompt: Optional[int] = 1,\n",
        "    generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
        "    latents: Optional[torch.FloatTensor] = None,\n",
        "    prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "    pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "    output_type: Optional[str] = \"pil\",\n",
        "    return_dict: bool = True,\n",
        "    joint_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
        "    max_sequence_length: int = 512,\n",
        "    good_vae: Optional[Any] = None,\n",
        "):\n",
        "    height = height or self.default_sample_size * self.vae_scale_factor\n",
        "    width = width or self.default_sample_size * self.vae_scale_factor\n",
        "\n",
        "    # 1. Check inputs\n",
        "    self.check_inputs(\n",
        "        prompt,\n",
        "        prompt_2,\n",
        "        height,\n",
        "        width,\n",
        "        prompt_embeds=prompt_embeds,\n",
        "        pooled_prompt_embeds=pooled_prompt_embeds,\n",
        "        max_sequence_length=max_sequence_length,\n",
        "    )\n",
        "\n",
        "    self._guidance_scale = guidance_scale\n",
        "    self._joint_attention_kwargs = joint_attention_kwargs\n",
        "    self._interrupt = False\n",
        "\n",
        "    # 2. Define call parameters\n",
        "    batch_size = 1 if isinstance(prompt, str) else len(prompt)\n",
        "    device = self._execution_device\n",
        "\n",
        "    # 3. Encode prompt\n",
        "    lora_scale = joint_attention_kwargs.get(\"scale\", None) if joint_attention_kwargs is not None else None\n",
        "    prompt_embeds, pooled_prompt_embeds, text_ids = self.encode_prompt(\n",
        "        prompt=prompt,\n",
        "        prompt_2=prompt_2,\n",
        "        prompt_embeds=prompt_embeds,\n",
        "        pooled_prompt_embeds=pooled_prompt_embeds,\n",
        "        device=device,\n",
        "        num_images_per_prompt=num_images_per_prompt,\n",
        "        max_sequence_length=max_sequence_length,\n",
        "        lora_scale=lora_scale,\n",
        "    )\n",
        "\n",
        "    # 4. Prepare latent variables\n",
        "    num_channels_latents = self.transformer.config.in_channels // 4\n",
        "    latents, latent_image_ids = self.prepare_latents(\n",
        "        batch_size * num_images_per_prompt,\n",
        "        num_channels_latents,\n",
        "        height,\n",
        "        width,\n",
        "        prompt_embeds.dtype,\n",
        "        device,\n",
        "        generator,\n",
        "        latents,\n",
        "    )\n",
        "\n",
        "    # 5. Prepare timesteps\n",
        "    sigmas = np.linspace(1.0, 1 / num_inference_steps, num_inference_steps)\n",
        "    image_seq_len = latents.shape[1]\n",
        "    mu = calculate_shift(\n",
        "        image_seq_len,\n",
        "        self.scheduler.config.base_image_seq_len,\n",
        "        self.scheduler.config.max_image_seq_len,\n",
        "        self.scheduler.config.base_shift,\n",
        "        self.scheduler.config.max_shift,\n",
        "    )\n",
        "    timesteps, num_inference_steps = retrieve_timesteps(\n",
        "        self.scheduler,\n",
        "        num_inference_steps,\n",
        "        device,\n",
        "        timesteps,\n",
        "        sigmas,\n",
        "        mu=mu,\n",
        "    )\n",
        "    self._num_timesteps = len(timesteps)\n",
        "\n",
        "    # Handle guidance\n",
        "    guidance = torch.full([1], guidance_scale, device=device, dtype=torch.float32).expand(latents.shape[0]) if self.transformer.config.guidance_embeds else None\n",
        "\n",
        "    # 6. Denoising loop\n",
        "    for i, t in enumerate(timesteps):\n",
        "        if self._interrupt:\n",
        "            continue\n",
        "\n",
        "        timestep = t.expand(latents.shape[0]).to(latents.dtype)\n",
        "\n",
        "        noise_pred = self.transformer(\n",
        "            hidden_states=latents,\n",
        "            timestep=timestep / 1000,\n",
        "            guidance=guidance,\n",
        "            pooled_projections=pooled_prompt_embeds,\n",
        "            encoder_hidden_states=prompt_embeds,\n",
        "            txt_ids=text_ids,\n",
        "            img_ids=latent_image_ids,\n",
        "            joint_attention_kwargs=self.joint_attention_kwargs,\n",
        "            return_dict=False,\n",
        "        )[0]\n",
        "        latents = self.scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n",
        "\n",
        "        # Yield intermediate result\n",
        "        latents_for_image = self._unpack_latents(latents, height, width, self.vae_scale_factor)\n",
        "        latents_for_image = (latents_for_image / self.vae.config.scaling_factor) + self.vae.config.shift_factor\n",
        "        image = self.vae.decode(latents_for_image, return_dict=False)[0]\n",
        "        yield self.image_processor.postprocess(image, output_type=output_type)[0]\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Final image using good_vae\n",
        "    latents = self._unpack_latents(latents, height, width, self.vae_scale_factor)\n",
        "    latents = (latents / good_vae.config.scaling_factor) + good_vae.config.shift_factor\n",
        "    image = good_vae.decode(latents, return_dict=False)[0]\n",
        "    self.maybe_free_model_hooks()\n",
        "    torch.cuda.empty_cache()\n",
        "    yield self.image_processor.postprocess(image, output_type=output_type)[0]\n",
        "\n",
        "\n",
        "# Use the Llama model to answer a question\n",
        "llm_result = llm.create_chat_completion(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What is the capital of France?\"\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Display the result from the Llama model\n",
        "print(\"Llama Model Response: \", llm_result['choices'][0]['message']['content'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNcAKgxgZhGS",
        "outputId": "ec2a4d96-ceb4-45a9-8723-7b523779a09c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /root/.cache/huggingface/hub/models--ayan-sh003--phi3.5-phunction-calling-GGUF/snapshots/d2748d5f380f85ced5fd21033cf4490490ad7da8/./phi3.5-phunction-q5-k-m.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Phi 3.5 Mini Instruct\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = instruct\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Phi-3.5\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = mini\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                          general.file_type u32              = 17\n",
            "llama_model_loader: - kv  16:                           llama.vocab_size u32              = 32064\n",
            "llama_model_loader: - kv  17:                 llama.rope.dimension_count u32              = 96\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 32009\n",
            "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if 'role' in messages[0] %}{% for ...\n",
            "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q5_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens cache size = 14\n",
            "llm_load_vocab: token to piece cache size = 0.1685 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32064\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 3072\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_rot            = 96\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 96\n",
            "llm_load_print_meta: n_embd_head_v    = 96\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
            "llm_load_print_meta: n_embd_v_gqa     = 3072\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 8192\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
            "llm_load_print_meta: model params     = 3.82 B\n",
            "llm_load_print_meta: model size       = 2.53 GiB (5.68 BPW) \n",
            "llm_load_print_meta: general.name     = Phi 3.5 Mini Instruct\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 32009 '<|placeholder6|>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: EOT token        = 32007 '<|end|>'\n",
            "llm_load_print_meta: max token length = 48\n",
            "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
            "llm_load_tensors:        CPU buffer size =  2588.53 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   192.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  192.00 MiB, K (f16):   96.00 MiB, V (f16):   96.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    68.63 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{% if 'role' in messages[0] %}{% for message in messages %}{% if message['role'] == 'user' %}{{'<|user|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\\n' + message['content'] + '<|end|>\\n'}}{% else %}{{'<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\\n' }}{% endif %}{% else %}{% for message in messages %}{% if message['from'] == 'human' %}{{'<|user|>\\n' + message['value'] + '<|end|>\\n'}}{% elif message['from'] == 'gpt' %}{{'<|assistant|>\\n' + message['value'] + '<|end|>\\n'}}{% else %}{{'<|' + message['from'] + '|>\\n' + message['value'] + '<|end|>\\n'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\\n' }}{% endif %}{% endif %}\", 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_space_prefix': 'false', 'llama.rope.dimension_count': '96', 'llama.vocab_size': '32064', 'general.file_type': '17', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'general.architecture': 'llama', 'tokenizer.ggml.padding_token_id': '32009', 'general.basename': 'Phi-3.5', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'default', 'llama.context_length': '131072', 'general.name': 'Phi 3.5 Mini Instruct', 'general.organization': 'Unsloth', 'general.finetune': 'instruct', 'general.type': 'model', 'general.size_label': 'mini', 'tokenizer.ggml.add_bos_token': 'false', 'llama.embedding_length': '3072', 'llama.feed_forward_length': '8192', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {% if 'role' in messages[0] %}{% for message in messages %}{% if message['role'] == 'user' %}{{'<|user|>\n",
            "' + message['content'] + '<|end|>\n",
            "'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n",
            "' + message['content'] + '<|end|>\n",
            "'}}{% else %}{{'<|' + message['role'] + '|>\n",
            "' + message['content'] + '<|end|>\n",
            "'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
            "' }}{% endif %}{% else %}{% for message in messages %}{% if message['from'] == 'human' %}{{'<|user|>\n",
            "' + message['value'] + '<|end|>\n",
            "'}}{% elif message['from'] == 'gpt' %}{{'<|assistant|>\n",
            "' + message['value'] + '<|end|>\n",
            "'}}{% else %}{{'<|' + message['from'] + '|>\n",
            "' + message['value'] + '<|end|>\n",
            "'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
            "' }}{% endif %}{% endif %}\n",
            "Using chat eos_token: <|endoftext|>\n",
            "Using chat bos_token: <s>\n",
            "\n",
            "llama_print_timings:        load time =    5241.34 ms\n",
            "llama_print_timings:      sample time =       0.44 ms /     8 runs   (    0.06 ms per token, 18140.59 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5241.26 ms /    13 tokens (  403.17 ms per token,     2.48 tokens per second)\n",
            "llama_print_timings:        eval time =    3436.32 ms /     7 runs   (  490.90 ms per token,     2.04 tokens per second)\n",
            "llama_print_timings:       total time =    8685.58 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Llama Model Response:  The capital of France is Paris.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Conclusion:\n",
        "\n",
        "The Phi 3.5 Mini Instruct model, based on the LLaMA architecture, was successfully loaded and initialized with detailed configuration metadata. The model's performance was evaluated, demonstrating efficient token processing and accurate inference capabilities. The final output confirms that the model functions correctly in generating appropriate responses, making it a reliable tool for similar natural language processing tasks."
      ],
      "metadata": {
        "id": "rDZJZjGCcutN"
      }
    }
  ]
}